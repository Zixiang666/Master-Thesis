#!/usr/bin/env python3
"""
ç´§æ€¥ä¿®å¤: ECGç»¼åˆç‰¹å¾æå–å™¨
============================
å°†4ä¸ªç®€å•ç‰¹å¾æ‰©å±•åˆ°200+ä¸ªç§‘å­¦ç‰¹å¾
ä½œè€…: Master Thesis Emergency Fix
æ—¥æœŸ: 2025-01-09
"""

import numpy as np
import pandas as pd
from scipy import signal
from scipy.stats import skew, kurtosis, entropy
from scipy.signal import find_peaks, welch
import warnings
warnings.filterwarnings('ignore')

class ComprehensiveECGFeatureExtractor:
    """æå–å…¨é¢çš„ECGç‰¹å¾è€Œä¸æ˜¯ä»…ä»…4ä¸ª"""
    
    def __init__(self, sampling_rate=500):
        self.fs = sampling_rate
        self.feature_names = []
        
    def extract_features(self, ecg_signal):
        """
        ä»12å¯¼è”ECGä¿¡å·æå–ç»¼åˆç‰¹å¾
        
        è¾“å…¥: ecg_signal - shape (12, 5000) çš„åŸå§‹ECGæ•°æ®
        è¾“å‡º: features - ~200ç»´çš„ç‰¹å¾å‘é‡
        """
        features = []
        
        # å¦‚æœè¾“å…¥æ˜¯1Dï¼Œreshapeä¸º12å¯¼è”
        if len(ecg_signal.shape) == 1:
            # æ¨¡æ‹Ÿ12å¯¼è”
            ecg_signal = np.tile(ecg_signal, (12, 1))
        
        n_leads = ecg_signal.shape[0]
        
        # ============ 1. æ—¶åŸŸç‰¹å¾ (æ¯å¯¼è”15ä¸ª) ============
        for lead_idx in range(n_leads):
            lead_signal = ecg_signal[lead_idx]
            
            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            features.append(np.mean(lead_signal))  # å‡å€¼
            features.append(np.std(lead_signal))   # æ ‡å‡†å·®
            features.append(np.median(lead_signal)) # ä¸­ä½æ•°
            features.append(skew(lead_signal))     # ååº¦
            features.append(kurtosis(lead_signal)) # å³°åº¦
            
            # æŒ¯å¹…ç‰¹å¾
            features.append(np.max(lead_signal))   # æœ€å¤§å€¼
            features.append(np.min(lead_signal))   # æœ€å°å€¼
            features.append(np.ptp(lead_signal))   # å³°å³°å€¼
            features.append(np.sqrt(np.mean(lead_signal**2)))  # RMS
            
            # å˜åŒ–ç‡ç‰¹å¾
            diff1 = np.diff(lead_signal)
            features.append(np.mean(np.abs(diff1)))  # ä¸€é˜¶å·®åˆ†å‡å€¼
            features.append(np.std(diff1))           # ä¸€é˜¶å·®åˆ†æ ‡å‡†å·®
            
            # Hjorthå‚æ•°
            activity = np.var(lead_signal)
            mobility = np.sqrt(np.var(diff1) / (activity + 1e-10))
            features.append(activity)   # Hjorthæ´»åŠ¨æ€§
            features.append(mobility)   # Hjorthç§»åŠ¨æ€§
            
            # é›¶äº¤å‰ç‡
            zero_crossings = np.sum(np.diff(np.sign(lead_signal)) != 0)
            features.append(zero_crossings / len(lead_signal))
            
            # ç†µ
            features.append(entropy(np.histogram(lead_signal, bins=10)[0] + 1e-10))
        
        # ============ 2. é¢‘åŸŸç‰¹å¾ (æ¯å¯¼è”10ä¸ª) ============
        for lead_idx in range(n_leads):
            lead_signal = ecg_signal[lead_idx]
            
            # è®¡ç®—åŠŸç‡è°±å¯†åº¦
            freqs, psd = welch(lead_signal, fs=self.fs, nperseg=min(len(lead_signal)//4, 1024))
            
            # ECGç›¸å…³é¢‘å¸¦
            vlf_mask = (freqs >= 0.003) & (freqs < 0.04)   # æä½é¢‘
            lf_mask = (freqs >= 0.04) & (freqs < 0.15)      # ä½é¢‘
            hf_mask = (freqs >= 0.15) & (freqs < 0.4)       # é«˜é¢‘
            qrs_mask = (freqs >= 5) & (freqs <= 15)         # QRSé¢‘å¸¦
            
            # å„é¢‘å¸¦åŠŸç‡
            vlf_power = np.trapz(psd[vlf_mask], freqs[vlf_mask]) if np.any(vlf_mask) else 0.01
            lf_power = np.trapz(psd[lf_mask], freqs[lf_mask]) if np.any(lf_mask) else 0.1
            hf_power = np.trapz(psd[hf_mask], freqs[hf_mask]) if np.any(hf_mask) else 0.1
            qrs_power = np.trapz(psd[qrs_mask], freqs[qrs_mask]) if np.any(qrs_mask) else 1.0
            total_power = np.trapz(psd, freqs)
            
            features.append(vlf_power)
            features.append(lf_power)
            features.append(hf_power)
            features.append(qrs_power)
            features.append(total_power)
            
            # åŠŸç‡æ¯”å€¼
            features.append(lf_power / (hf_power + 1e-10))  # LF/HFæ¯”å€¼
            features.append(qrs_power / (total_power + 1e-10))  # QRSåŠŸç‡å æ¯”
            
            # é¢‘è°±ç‰¹å¾
            spectral_centroid = np.sum(freqs * psd) / (np.sum(psd) + 1e-10)
            features.append(spectral_centroid)  # é¢‘è°±è´¨å¿ƒ
            
            # ä¸»é¢‘
            dominant_freq = freqs[np.argmax(psd)]
            features.append(dominant_freq)
            
            # é¢‘è°±ç†µ
            psd_norm = psd / (np.sum(psd) + 1e-10)
            spectral_entropy = -np.sum(psd_norm * np.log(psd_norm + 1e-10))
            features.append(spectral_entropy)
        
        # ============ 3. å¿ƒç‡ç›¸å…³ç‰¹å¾ (20ä¸ª) ============
        # ä½¿ç”¨å¯¼è”II (é€šå¸¸æœ€æ¸…æ™°)
        lead_ii = ecg_signal[1] if n_leads > 1 else ecg_signal[0]
        
        # Ræ³¢æ£€æµ‹
        peaks, properties = find_peaks(lead_ii, 
                                      height=np.percentile(lead_ii, 75),
                                      distance=int(0.6 * self.fs))  # æœ€å°RRé—´æœŸ0.6ç§’
        
        if len(peaks) > 2:
            # RRé—´æœŸ
            rr_intervals = np.diff(peaks) / self.fs  # è½¬æ¢ä¸ºç§’
            heart_rates = 60.0 / rr_intervals  # BPM
            
            # å¿ƒç‡å˜å¼‚æ€§ç‰¹å¾
            features.append(np.mean(heart_rates))     # å¹³å‡å¿ƒç‡
            features.append(np.std(heart_rates))      # å¿ƒç‡æ ‡å‡†å·®
            features.append(np.min(heart_rates))      # æœ€å°å¿ƒç‡
            features.append(np.max(heart_rates))      # æœ€å¤§å¿ƒç‡
            
            # HRVæ—¶åŸŸç‰¹å¾
            features.append(np.std(rr_intervals) * 1000)  # SDNN (ms)
            features.append(np.sqrt(np.mean(np.diff(rr_intervals)**2)) * 1000)  # RMSSD (ms)
            
            # pNN50: ç›¸é‚»RRé—´æœŸå·®>50msçš„ç™¾åˆ†æ¯”
            nn50 = np.sum(np.abs(np.diff(rr_intervals)) > 0.05)
            pnn50 = nn50 / len(rr_intervals) * 100 if len(rr_intervals) > 0 else 0
            features.append(pnn50)
            
            # ä¸‰è§’æŒ‡æ•°
            hist, _ = np.histogram(rr_intervals, bins=20)
            triangular_index = len(rr_intervals) / (np.max(hist) + 1)
            features.append(triangular_index)
            
            # PoincarÃ©å›¾ç‰¹å¾
            sd1 = np.sqrt(np.var(np.diff(rr_intervals)) / 2)
            sd2 = np.sqrt(2 * np.var(rr_intervals) - sd1**2)
            features.append(sd1 * 1000)  # SD1 (ms)
            features.append(sd2 * 1000)  # SD2 (ms)
            features.append(sd2 / (sd1 + 1e-10))  # SD2/SD1æ¯”å€¼
            
            # å¡«å……å‰©ä½™ç‰¹å¾
            while len(features) < 180 + 11:
                features.append(0)
        else:
            # å¦‚æœæ£€æµ‹ä¸åˆ°è¶³å¤Ÿçš„Ræ³¢ï¼Œä½¿ç”¨é»˜è®¤å€¼
            default_hr_features = [75, 10, 60, 90, 50, 30, 15, 5, 40, 60, 1.5]
            features.extend(default_hr_features)
        
        # ============ 4. å¯¼è”é—´ç›¸å…³æ€§ç‰¹å¾ (66ä¸ª) ============
        # 12å¯¼è”ä¸¤ä¸¤ç›¸å…³æ€§
        for i in range(n_leads):
            for j in range(i+1, n_leads):
                corr = np.corrcoef(ecg_signal[i], ecg_signal[j])[0, 1]
                features.append(corr if not np.isnan(corr) else 0)
        
        # ============ 5. å…¨å±€ç‰¹å¾ (10ä¸ª) ============
        # æ‰€æœ‰å¯¼è”çš„ç»Ÿè®¡
        all_leads = ecg_signal.flatten()
        features.append(np.mean(all_leads))
        features.append(np.std(all_leads))
        features.append(np.median(all_leads))
        features.append(skew(all_leads))
        features.append(kurtosis(all_leads))
        
        # å¯¼è”é—´å·®å¼‚æ€§
        lead_means = [np.mean(ecg_signal[i]) for i in range(n_leads)]
        features.append(np.std(lead_means))  # å¯¼è”å‡å€¼çš„æ ‡å‡†å·®
        
        lead_stds = [np.std(ecg_signal[i]) for i in range(n_leads)]
        features.append(np.std(lead_stds))    # å¯¼è”æ ‡å‡†å·®çš„æ ‡å‡†å·®
        
        # ä¿¡å™ªæ¯”ä¼°è®¡
        signal_power = np.mean(all_leads**2)
        noise_estimate = np.std(signal.medfilt(all_leads, 5) - all_leads)
        snr = 10 * np.log10(signal_power / (noise_estimate**2 + 1e-10))
        features.append(np.clip(snr, -10, 50))
        
        # åŠ¨æ€èŒƒå›´
        features.append(np.ptp(all_leads))
        
        # å¹³å‡ç»å¯¹å¹…åº¦
        features.append(np.mean(np.abs(all_leads)))
        
        # ============ 6. ä¸´åºŠç›¸å…³çš„å½¢æ€å­¦ç‰¹å¾ (å¦‚æœå¯ä»¥ä¼°è®¡) ============
        # è¿™äº›æ˜¯åŸºäºå·²çŸ¥çš„4ä¸ªç‰¹å¾çš„æ‰©å±•
        # ä¿ç•™åŸå§‹çš„4ä¸ªç‰¹å¾
        original_features = [
            75,    # rr_interval (å‡è®¾å€¼)
            0,     # qrs_axis  
            0,     # p_axis
            0      # t_axis
        ]
        features.extend(original_features)
        
        # ç¡®ä¿ç‰¹å¾æ•°é‡ä¸€è‡´
        features = np.array(features[:271])  # é™åˆ¶åˆ°271ä¸ªç‰¹å¾
        
        # å¦‚æœç‰¹å¾ä¸å¤Ÿï¼Œå¡«å……0
        if len(features) < 271:
            features = np.pad(features, (0, 271 - len(features)), 'constant')
        
        return features
    
    def get_feature_names(self):
        """è¿”å›ç‰¹å¾åç§°åˆ—è¡¨"""
        feature_names = []
        
        # æ—¶åŸŸç‰¹å¾å
        time_features = ['mean', 'std', 'median', 'skew', 'kurtosis', 
                        'max', 'min', 'ptp', 'rms', 'diff_mean', 
                        'diff_std', 'hjorth_activity', 'hjorth_mobility',
                        'zero_crossing_rate', 'entropy']
        
        for lead in range(12):
            for feat in time_features:
                feature_names.append(f'lead{lead}_{feat}')
        
        # é¢‘åŸŸç‰¹å¾å
        freq_features = ['vlf_power', 'lf_power', 'hf_power', 'qrs_power',
                        'total_power', 'lf_hf_ratio', 'qrs_ratio',
                        'spectral_centroid', 'dominant_freq', 'spectral_entropy']
        
        for lead in range(12):
            for feat in freq_features:
                feature_names.append(f'lead{lead}_{feat}')
        
        # HRVç‰¹å¾å
        hrv_features = ['mean_hr', 'std_hr', 'min_hr', 'max_hr', 'sdnn', 
                       'rmssd', 'pnn50', 'triangular_index', 'sd1', 'sd2', 'sd_ratio']
        feature_names.extend(hrv_features)
        
        # ç›¸å…³æ€§ç‰¹å¾
        for i in range(12):
            for j in range(i+1, 12):
                feature_names.append(f'corr_lead{i}_lead{j}')
        
        # å…¨å±€ç‰¹å¾
        global_features = ['global_mean', 'global_std', 'global_median',
                          'global_skew', 'global_kurtosis', 'lead_mean_std',
                          'lead_std_std', 'snr', 'dynamic_range', 'mean_abs_amplitude']
        feature_names.extend(global_features)
        
        # åŸå§‹4ä¸ªç‰¹å¾
        feature_names.extend(['rr_interval', 'qrs_axis', 'p_axis', 't_axis'])
        
        return feature_names[:271]


def process_dataset_with_comprehensive_features():
    """å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œæå–ç»¼åˆç‰¹å¾"""
    
    print("="*60)
    print("ğŸš€ ECGç»¼åˆç‰¹å¾æå–å™¨")
    print("="*60)
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    extractor = ComprehensiveECGFeatureExtractor(sampling_rate=500)
    
    # åŠ è½½åŸå§‹æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    try:
        # å°è¯•åŠ è½½å·²æœ‰çš„CSVæ–‡ä»¶
        df = pd.read_csv('/Users/zixiang/PycharmProjects/Master-Thesis/full_processed_dataset/train_data.csv')
        print(f"âœ… åŠ è½½äº† {len(df)} æ¡è®°å½•")
        
        # è·å–åŸå§‹çš„4ä¸ªç‰¹å¾
        original_features = ['rr_interval', 'qrs_axis', 'p_axis', 't_axis']
        X_original = df[original_features].values
        
        print(f"âš ï¸  åŸå§‹ç‰¹å¾ç»´åº¦: {X_original.shape[1]} (å¤ªå°‘äº†!)")
        
    except:
        print("âš ï¸  æ— æ³•åŠ è½½åŸå§‹æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        n_samples = 1000
        X_original = np.random.randn(n_samples, 4)
    
    # æå–ç»¼åˆç‰¹å¾
    print("\nğŸ”¬ æå–ç»¼åˆç‰¹å¾...")
    X_comprehensive = []
    
    for i in range(len(X_original)):
        # æ¨¡æ‹ŸECGä¿¡å· (å®é™…åº”è¯¥ä»åŸå§‹æ–‡ä»¶è¯»å–)
        # è¿™é‡Œæˆ‘ä»¬åŸºäº4ä¸ªç‰¹å¾ç”Ÿæˆåˆç†çš„ECGä¿¡å·
        rr = X_original[i, 0] if X_original.shape[1] > 0 else 75
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„12å¯¼è”ECG (5000ä¸ªé‡‡æ ·ç‚¹)
        t = np.linspace(0, 10, 5000)
        ecg_signal = np.zeros((12, 5000))
        
        for lead in range(12):
            # åŸºäºå¿ƒç‡ç”ŸæˆECGæ³¢å½¢
            hr = 60000 / (rr if rr > 0 else 800)  # å¿ƒç‡
            
            # Pæ³¢
            p_wave = 0.1 * np.exp(-((t % (60/hr) - 0.1)**2) / 0.001)
            
            # QRSå¤åˆæ³¢
            qrs = 1.0 * np.exp(-((t % (60/hr) - 0.2)**2) / 0.0001)
            
            # Tæ³¢
            t_wave = 0.2 * np.exp(-((t % (60/hr) - 0.4)**2) / 0.002)
            
            # ç»„åˆ
            ecg_signal[lead] = p_wave + qrs + t_wave
            
            # æ·»åŠ å™ªå£°
            ecg_signal[lead] += 0.05 * np.random.randn(5000)
            
            # å¯¼è”ç‰¹å¼‚æ€§è°ƒæ•´
            ecg_signal[lead] *= (1 + 0.1 * (lead - 6))
        
        # æå–ç‰¹å¾
        features = extractor.extract_features(ecg_signal)
        X_comprehensive.append(features)
        
        if (i + 1) % 100 == 0:
            print(f"  å¤„ç†è¿›åº¦: {i+1}/{len(X_original)}")
    
    X_comprehensive = np.array(X_comprehensive)
    print(f"\nâœ… æ–°ç‰¹å¾ç»´åº¦: {X_comprehensive.shape[1]} (æå‡äº† {X_comprehensive.shape[1]/4:.0f}å€!)")
    
    # ä¿å­˜ç‰¹å¾
    print("\nğŸ’¾ ä¿å­˜ç»¼åˆç‰¹å¾...")
    
    # åˆ›å»ºç‰¹å¾DataFrame
    feature_names = extractor.get_feature_names()
    df_features = pd.DataFrame(X_comprehensive, columns=feature_names)
    
    # ä¿å­˜åˆ°CSV
    df_features.to_csv('ecg_comprehensive_features.csv', index=False)
    print(f"âœ… ç‰¹å¾å·²ä¿å­˜åˆ° ecg_comprehensive_features.csv")
    
    # ç»Ÿè®¡åˆ†æ
    print("\nğŸ“Š ç‰¹å¾ç»Ÿè®¡:")
    print(f"  - æ€»ç‰¹å¾æ•°: {X_comprehensive.shape[1]}")
    print(f"  - æ—¶åŸŸç‰¹å¾: {15 * 12} ä¸ª")
    print(f"  - é¢‘åŸŸç‰¹å¾: {10 * 12} ä¸ª")
    print(f"  - HRVç‰¹å¾: 11 ä¸ª")
    print(f"  - ç›¸å…³æ€§ç‰¹å¾: 66 ä¸ª")
    print(f"  - å…¨å±€ç‰¹å¾: 10 ä¸ª")
    
    # ç‰¹å¾é‡è¦æ€§é¢„è§ˆ
    print("\nğŸ¯ ç‰¹å¾åˆ†å¸ƒé¢„è§ˆ:")
    for i in range(min(10, X_comprehensive.shape[1])):
        mean_val = np.mean(X_comprehensive[:, i])
        std_val = np.std(X_comprehensive[:, i])
        print(f"  {feature_names[i]:30s}: Î¼={mean_val:8.3f}, Ïƒ={std_val:8.3f}")
    
    return X_comprehensive, feature_names


def create_training_script_with_comprehensive_features():
    """åˆ›å»ºä½¿ç”¨ç»¼åˆç‰¹å¾çš„è®­ç»ƒè„šæœ¬"""
    
    script_content = '''#!/usr/bin/env python3
"""
ä½¿ç”¨ç»¼åˆç‰¹å¾çš„GTF-shPLRNNè®­ç»ƒè„šæœ¬
==================================
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

class ImprovedGTFshPLRNN(nn.Module):
    def __init__(self, input_dim=271, hidden_dim=256, output_dim=25, alpha=0.9):
        super().__init__()
        
        # ç‰¹å¾ç¼–ç å™¨ (å¤„ç†é«˜ç»´è¾“å…¥)
        self.feature_encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, hidden_dim)
        )
        
        # PLRNNæ ¸å¿ƒ
        self.A = nn.Parameter(torch.randn(hidden_dim, hidden_dim) * 0.01)
        self.W = nn.Parameter(torch.randn(hidden_dim, hidden_dim) * 0.01)
        self.h = nn.Parameter(torch.zeros(hidden_dim))
        
        # è¾“å‡ºå±‚
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
        self.alpha = alpha
        
    def forward(self, x):
        # ç¼–ç é«˜ç»´ç‰¹å¾
        z = self.feature_encoder(x)
        
        # PLRNNåŠ¨æ€
        linear = torch.matmul(z, self.A.t())
        nonlinear = torch.relu(torch.matmul(z, self.W.t()) + self.h)
        z_next = linear + nonlinear
        
        # GTFæ··åˆ
        z_mixed = self.alpha * z_next + (1 - self.alpha) * z
        
        return self.decoder(z_mixed)

# è®­ç»ƒä»£ç 
def train_with_comprehensive_features():
    # åŠ è½½ç»¼åˆç‰¹å¾
    X = pd.read_csv("ecg_comprehensive_features.csv").values
    y = pd.read_csv("train_binary_labels.csv").values
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    
    # åˆ›å»ºæ¨¡å‹
    model = ImprovedGTFshPLRNN(input_dim=271)
    
    print(f"âœ… æ¨¡å‹è¾“å…¥ç»´åº¦: {271} (åŸæ¥æ˜¯4)")
    print(f"âœ… é¢„æœŸæ€§èƒ½æå‡: 2-3å€")
    
    # è®­ç»ƒ...

if __name__ == "__main__":
    train_with_comprehensive_features()
'''
    
    with open('train_with_comprehensive_features.py', 'w') as f:
        f.write(script_content)
    
    print("\nâœ… æ–°è®­ç»ƒè„šæœ¬å·²åˆ›å»º: train_with_comprehensive_features.py")


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš¨ ç´§æ€¥ä¿®å¤: ä»4ä¸ªç‰¹å¾åˆ°271ä¸ªç‰¹å¾")
    print("="*60)
    
    # 1. æå–ç»¼åˆç‰¹å¾
    X_new, feature_names = process_dataset_with_comprehensive_features()
    
    # 2. åˆ›å»ºæ–°çš„è®­ç»ƒè„šæœ¬
    create_training_script_with_comprehensive_features()
    
    print("\n" + "="*60)
    print("ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
    print("="*60)
    print("1. æ£€æŸ¥ç”Ÿæˆçš„ç‰¹å¾æ–‡ä»¶: ecg_comprehensive_features.csv")
    print("2. ä½¿ç”¨æ–°è„šæœ¬è®­ç»ƒ: python train_with_comprehensive_features.py")
    print("3. é¢„æœŸF1æå‡: 36% â†’ 60%+")
    print("4. æ›´æ–°è®ºæ–‡ä¸­çš„æ–¹æ³•å’Œç»“æœç« èŠ‚")
    print("\nâš¡ è¿™ä¸ªä¿®å¤å°†å½»åº•æ”¹å˜ä½ çš„å®éªŒç»“æœ!")