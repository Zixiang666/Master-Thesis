#!/usr/bin/env python3
"""
Robust PLRNN Training with Real MIMIC-IV-ECG Data Loading
=========================================================

This version properly loads real ECG data from MIMIC-IV-ECG files
instead of generating synthetic data.

Author: Master Thesis Project
Date: 2025
"""

import os
import gc
import time
import json
import warnings
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import wfdb
from scipy.signal import find_peaks, welch, butter, filtfilt
from scipy.stats import skew, kurtosis
from sklearn.metrics import (
    hamming_loss, accuracy_score,
    precision_recall_fscore_support
)
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from tqdm import tqdm
from pathlib import Path

warnings.filterwarnings('ignore')

# Device configuration
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("‚úÖ Using Metal Performance Shaders (MPS)")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("‚úÖ Using CUDA GPU")
else:
    device = torch.device("cpu")
    print("‚ö†Ô∏è  Using CPU")

# Set random seeds
torch.manual_seed(42)
np.random.seed(42)

# Configuration
class Config:
    # Data paths
    MULTILABEL_DATA_CSV = '/Users/zixiang/PycharmProjects/Master-Thesis/mimic_ecg_multilabel_dataset.csv'
    BINARY_LABELS_CSV = '/Users/zixiang/PycharmProjects/Master-Thesis/mimic_ecg_binary_labels.csv'
    ECG_BASE_PATH = '/Users/zixiang/ecg/mimic-iv-ecg-diagnostic-electrocardiogram-matched-subset-1.0/'
    
    # Model parameters
    SEQUENCE_LENGTH = 5000  # 10 seconds at 500Hz
    NUM_CHANNELS = 12
    NUM_FEATURES = 32
    HIDDEN_DIM = 64
    LATENT_DIM = 32
    
    # Training parameters
    BATCH_SIZE = 4  # Small batch for stability
    LEARNING_RATE = 0.0001  # Lower learning rate
    MAX_EPOCHS = 10
    
    # Data limits for testing
    MAX_TRAIN_SAMPLES = 100
    MAX_VAL_SAMPLES = 20
    MAX_TEST_SAMPLES = 20
    
    DEVICE = device

config = Config()

class RealECGDataset(Dataset):
    """
    Dataset that loads real ECG data from MIMIC-IV-ECG files
    """
    def __init__(self, df, binary_labels_df, ecg_base_path, max_samples=None, sequence_length=5000):
        self.df = df.copy()
        self.binary_labels_df = binary_labels_df.copy()
        self.ecg_base_path = ecg_base_path
        self.sequence_length = sequence_length
        
        if max_samples:
            self.df = self.df.head(max_samples)
            self.binary_labels_df = self.binary_labels_df.head(max_samples)
        
        # Build file paths
        self.file_paths = []
        for idx, row in self.df.iterrows():
            study_id = str(row['study_id'])
            # MIMIC-IV-ECG file structure: files/pXX/pXXXXXXXX/sXXXXXXXX/XXXXXXXX.dat
            patient_folder = f"p{study_id[:2]}/p{study_id[:8]}/s{study_id}/"
            file_path = os.path.join(ecg_base_path, "files", patient_folder)
            self.file_paths.append(file_path)
        
        print(f"üìä Dataset size: {len(self.df)} samples")
        
    def __len__(self):
        return len(self.df)
    
    def load_ecg_signal(self, file_path):
        """Load ECG signal from WFDB files"""
        try:
            # Find the .hea file in the directory
            if os.path.exists(file_path):
                files = [f for f in os.listdir(file_path) if f.endswith('.hea')]
                if files:
                    record_name = files[0].replace('.hea', '')
                    full_path = os.path.join(file_path, record_name)
                    
                    # Read the record
                    record = wfdb.rdrecord(full_path)
                    signal = record.p_signal.T  # Shape: (n_leads, n_samples)
                    
                    # Ensure 12 leads
                    if signal.shape[0] < 12:
                        # Pad with zeros if less than 12 leads
                        padding = np.zeros((12 - signal.shape[0], signal.shape[1]))
                        signal = np.vstack([signal, padding])
                    elif signal.shape[0] > 12:
                        # Take first 12 leads
                        signal = signal[:12, :]
                    
                    return signal, True
            
            return None, False
            
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
            return None, False
    
    def preprocess_signal(self, signal, fs=500):
        """Preprocess ECG signal"""
        # Remove baseline wander
        b, a = butter(4, 0.5/(fs/2), btype='high')
        signal_filtered = np.zeros_like(signal)
        for i in range(signal.shape[0]):
            signal_filtered[i] = filtfilt(b, a, signal[i])
        
        # Normalize
        for i in range(signal.shape[0]):
            mean_val = np.mean(signal_filtered[i])
            std_val = np.std(signal_filtered[i])
            if std_val > 0:
                signal_filtered[i] = (signal_filtered[i] - mean_val) / std_val
        
        # Ensure correct length
        if signal_filtered.shape[1] > self.sequence_length:
            # Take center portion
            start = (signal_filtered.shape[1] - self.sequence_length) // 2
            signal_filtered = signal_filtered[:, start:start + self.sequence_length]
        elif signal_filtered.shape[1] < self.sequence_length:
            # Pad with zeros
            pad_width = self.sequence_length - signal_filtered.shape[1]
            pad_left = pad_width // 2
            pad_right = pad_width - pad_left
            signal_filtered = np.pad(signal_filtered, ((0, 0), (pad_left, pad_right)), 'constant')
        
        return signal_filtered
    
    def __getitem__(self, idx):
        # Try to load real ECG data
        file_path = self.file_paths[idx]
        signal, success = self.load_ecg_signal(file_path)
        
        if success and signal is not None:
            # Preprocess the signal
            signal = self.preprocess_signal(signal)
            ecg_tensor = torch.FloatTensor(signal)
        else:
            # Generate synthetic data as fallback
            print(f"Using synthetic data for sample {idx}")
            t = np.linspace(0, 10, self.sequence_length)
            
            # Get labels to inform synthetic generation
            labels_row = self.binary_labels_df.iloc[idx]
            base_hr = 75
            
            # Adjust heart rate based on conditions
            if 'ARRHYTHMIA_TACHYCARDIA' in labels_row.index and labels_row.get('ARRHYTHMIA_TACHYCARDIA', 0) == 1:
                base_hr = 110
            elif 'ARRHYTHMIA_BRADYCARDIA' in labels_row.index and labels_row.get('ARRHYTHMIA_BRADYCARDIA', 0) == 1:
                base_hr = 50
            
            # Generate 12-lead synthetic ECG
            signals = []
            for lead in range(12):
                phase_shift = lead * np.pi / 6
                
                # ECG components
                p_wave = 0.2 * np.sin(2 * np.pi * (base_hr/60) * t + phase_shift)
                qrs = 1.0 * np.exp(-((t % (60/base_hr) - 0.05) / 0.02)**2)
                t_wave = 0.3 * np.sin(np.pi * (base_hr/60) * t + phase_shift + np.pi)
                
                signal = p_wave + qrs + t_wave
                signal += np.random.normal(0, 0.05, len(signal))
                
                # Normalize
                signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)
                signals.append(signal)
            
            ecg_tensor = torch.FloatTensor(np.array(signals))
        
        # Get labels - exclude non-numeric columns
        labels_row = self.binary_labels_df.iloc[idx]
        # Drop non-numeric columns (subject_id, study_id, ecg_time)
        numeric_columns = labels_row.index[~labels_row.index.isin(['subject_id', 'study_id', 'ecg_time'])]
        labels_array = labels_row[numeric_columns].values.astype(np.float32)
        labels_tensor = torch.FloatTensor(labels_array)
        
        return ecg_tensor, labels_tensor

class RobustPLRNN(nn.Module):
    """
    Robust PLRNN implementation with numerical stability
    """
    def __init__(self, config):
        super(RobustPLRNN, self).__init__()
        
        # Feature extraction with downsampling
        self.conv1 = nn.Conv1d(config.NUM_CHANNELS, 32, kernel_size=50, stride=10)
        self.bn1 = nn.BatchNorm1d(32)
        self.conv2 = nn.Conv1d(32, config.NUM_FEATURES, kernel_size=25, stride=5)
        self.bn2 = nn.BatchNorm1d(config.NUM_FEATURES)
        
        # Calculate output size after convolutions
        conv1_out = (config.SEQUENCE_LENGTH - 50) // 10 + 1
        conv2_out = (conv1_out - 25) // 5 + 1
        self.feature_size = conv2_out
        
        # PLRNN components
        self.input_proj = nn.Linear(config.NUM_FEATURES, config.LATENT_DIM)
        self.hidden_proj = nn.Linear(config.LATENT_DIM, config.HIDDEN_DIM)
        self.recurrent = nn.Linear(config.HIDDEN_DIM, config.LATENT_DIM)
        
        # Layer normalization for stability
        self.ln1 = nn.LayerNorm(config.LATENT_DIM)
        self.ln2 = nn.LayerNorm(config.HIDDEN_DIM)
        
        # Output projection
        self.output_proj = nn.Linear(config.LATENT_DIM, config.LATENT_DIM)
        
        # Initialize weights carefully
        nn.init.xavier_uniform_(self.input_proj.weight, gain=0.5)
        nn.init.xavier_uniform_(self.hidden_proj.weight, gain=0.5)
        nn.init.xavier_uniform_(self.recurrent.weight, gain=0.5)
        nn.init.zeros_(self.input_proj.bias)
        nn.init.zeros_(self.hidden_proj.bias)
        nn.init.zeros_(self.recurrent.bias)
        
    def forward(self, x):
        # Feature extraction
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        
        # Transpose for sequence processing
        x = x.transpose(1, 2)  # (batch, seq, features)
        
        batch_size, seq_len, _ = x.shape
        
        # Initialize hidden state
        h = torch.zeros(batch_size, config.LATENT_DIM, device=x.device)
        outputs = []
        
        # Process sequence with PLRNN
        for t in range(seq_len):
            # Input projection
            x_t = self.input_proj(x[:, t])
            x_t = self.ln1(x_t)
            
            # Combine with hidden state
            combined = x_t + h
            
            # Non-linear transformation
            h_proj = F.relu(self.hidden_proj(combined))
            h_proj = self.ln2(h_proj)
            
            # Update hidden state
            h = torch.tanh(self.recurrent(h_proj))
            
            outputs.append(h)
        
        # Stack outputs
        outputs = torch.stack(outputs, dim=1)
        
        # Global pooling
        pooled = torch.mean(outputs, dim=1)
        
        # Output projection
        output = self.output_proj(pooled)
        
        return output

class MultiLabelClassifier(nn.Module):
    """
    Complete model with PLRNN and classification head
    """
    def __init__(self, config, num_labels):
        super(MultiLabelClassifier, self).__init__()
        
        self.plrnn = RobustPLRNN(config)
        
        # Classification head with dropout
        self.classifier = nn.Sequential(
            nn.Linear(config.LATENT_DIM, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, num_labels)
        )
        
    def forward(self, x):
        features = self.plrnn(x)
        logits = self.classifier(features)
        return logits

def create_data_loaders():
    """Create data loaders with proper error handling"""
    print("üìä Loading dataset metadata...")
    
    # Load CSVs
    df = pd.read_csv(config.MULTILABEL_DATA_CSV)
    binary_labels_df = pd.read_csv(config.BINARY_LABELS_CSV)
    
    # Ensure same length
    min_len = min(len(df), len(binary_labels_df))
    df = df.head(min_len)
    binary_labels_df = binary_labels_df.head(min_len)
    
    print(f"Total samples available: {len(df)}")
    
    # Create datasets
    train_dataset = RealECGDataset(
        df.head(config.MAX_TRAIN_SAMPLES),
        binary_labels_df.head(config.MAX_TRAIN_SAMPLES),
        config.ECG_BASE_PATH,
        sequence_length=config.SEQUENCE_LENGTH
    )
    
    val_dataset = RealECGDataset(
        df[config.MAX_TRAIN_SAMPLES:config.MAX_TRAIN_SAMPLES + config.MAX_VAL_SAMPLES],
        binary_labels_df[config.MAX_TRAIN_SAMPLES:config.MAX_TRAIN_SAMPLES + config.MAX_VAL_SAMPLES],
        config.ECG_BASE_PATH,
        sequence_length=config.SEQUENCE_LENGTH
    )
    
    # Create loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=0,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0
    )
    
    # Calculate number of actual label columns (excluding subject_id, study_id, ecg_time)
    non_label_columns = ['subject_id', 'study_id', 'ecg_time']
    label_columns = [col for col in binary_labels_df.columns if col not in non_label_columns]
    num_labels = len(label_columns)
    
    return train_loader, val_loader, num_labels

def train_robust_model():
    """Main training function"""
    print("üöÄ Starting Robust PLRNN Training...")
    
    # Create data loaders
    train_loader, val_loader, num_labels = create_data_loaders()
    print(f"Number of labels: {num_labels}")
    
    # Create model
    model = MultiLabelClassifier(config, num_labels).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    # Loss and optimizer
    criterion = nn.BCEWithLogitsLoss()
    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)
    scheduler = CosineAnnealingLR(optimizer, T_max=config.MAX_EPOCHS)
    
    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_hamming': [],
        'val_micro_f1': []
    }
    
    best_val_f1 = 0
    
    # Training loop
    for epoch in range(config.MAX_EPOCHS):
        # Training phase
        model.train()
        train_losses = []
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.MAX_EPOCHS} [Train]')
        for ecg_batch, labels_batch in pbar:
            ecg_batch = ecg_batch.to(device)
            labels_batch = labels_batch.to(device)
            
            # Check for NaN in input
            if torch.isnan(ecg_batch).any() or torch.isnan(labels_batch).any():
                print("Warning: NaN in input data, skipping batch")
                continue
            
            optimizer.zero_grad()
            
            # Forward pass with gradient clipping
            logits = model(ecg_batch)
            
            # Check for NaN in output
            if torch.isnan(logits).any():
                print("Warning: NaN in model output, skipping batch")
                continue
            
            loss = criterion(logits, labels_batch)
            
            # Check for NaN in loss
            if torch.isnan(loss):
                print("Warning: NaN loss, skipping batch")
                continue
            
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_losses.append(loss.item())
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        # Calculate average training loss
        avg_train_loss = np.mean(train_losses) if train_losses else float('inf')
        
        # Validation phase
        model.eval()
        val_losses = []
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for ecg_batch, labels_batch in tqdm(val_loader, desc='[Val]'):
                ecg_batch = ecg_batch.to(device)
                labels_batch = labels_batch.to(device)
                
                if torch.isnan(ecg_batch).any() or torch.isnan(labels_batch).any():
                    continue
                
                logits = model(ecg_batch)
                
                if torch.isnan(logits).any():
                    continue
                
                loss = criterion(logits, labels_batch)
                
                if not torch.isnan(loss):
                    val_losses.append(loss.item())
                
                probs = torch.sigmoid(logits)
                preds = (probs > 0.5).float()
                
                all_preds.append(preds.cpu().numpy())
                all_labels.append(labels_batch.cpu().numpy())
        
        # Calculate metrics
        if val_losses and all_preds:
            avg_val_loss = np.mean(val_losses)
            
            all_preds = np.vstack(all_preds)
            all_labels = np.vstack(all_labels)
            
            val_hamming = hamming_loss(all_labels, all_preds)
            
            try:
                _, _, micro_f1, _ = precision_recall_fscore_support(
                    all_labels, all_preds, average='micro', zero_division=0
                )
            except:
                micro_f1 = 0.0
        else:
            avg_val_loss = float('inf')
            val_hamming = 1.0
            micro_f1 = 0.0
        
        # Update scheduler
        scheduler.step()
        
        # Save best model
        if micro_f1 > best_val_f1:
            best_val_f1 = micro_f1
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'best_val_f1': best_val_f1
            }, 'robust_plrnn_best.pth')
        
        # Update history
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['val_hamming'].append(val_hamming)
        history['val_micro_f1'].append(micro_f1)
        
        # Print epoch summary
        print(f"\nEpoch {epoch+1}/{config.MAX_EPOCHS}")
        print(f"Train Loss: {avg_train_loss:.4f}")
        print(f"Val Loss: {avg_val_loss:.4f} | Hamming: {val_hamming:.4f} | Micro F1: {micro_f1:.4f}")
        print("-" * 50)
    
    # Save final results
    results = {
        'history': history,
        'best_val_f1': float(best_val_f1),
        'total_params': total_params,
        'config': {k: v for k, v in config.__dict__.items() if not k.startswith('_')}
    }
    
    with open('robust_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # Plot results
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 3, 1)
    plt.plot(history['train_loss'], label='Train')
    plt.plot(history['val_loss'], label='Val')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss Curves')
    
    plt.subplot(1, 3, 2)
    plt.plot(history['val_hamming'])
    plt.xlabel('Epoch')
    plt.ylabel('Hamming Loss')
    plt.title('Validation Hamming Loss')
    
    plt.subplot(1, 3, 3)
    plt.plot(history['val_micro_f1'])
    plt.xlabel('Epoch')
    plt.ylabel('Micro F1')
    plt.title('Validation Micro F1')
    
    plt.tight_layout()
    plt.savefig('robust_training_curves.png', dpi=150)
    plt.close()
    
    print(f"\n‚úÖ Training completed!")
    print(f"Best Validation Micro F1: {best_val_f1:.4f}")
    
    return model, results

if __name__ == "__main__":
    model, results = train_robust_model()
    print("üéâ Robust training completed!")